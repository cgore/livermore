\section{Time Series Prediction}
\vspace*{-\baselineskip}

\subsection{ARIMA and Other Statistical Methods}
ARIMA, the \emph{autoregressive integrated moving average}, is a common and very powerful statistical method often used in econometric models that can help forecast and estimate what is going to happen in the future.
The ARIMA time series analysis uses lags and shifts in the historical data to uncover patterns (e.g., moving averages, seasonality) and predict the future \cite{BoxJenkins:1994}.
The ARIMA model was first developed in the late 1960s but was not systemized until the work of Box and Jenkins in 1976 \cite{BoxJenkins:1976}.
ARIMA can be more complex to use than other statistical forecasting techniques, although when implemented properly can be quite powerful and flexible.  ARIMA is a method for determining two things:
\begin{enumerate}
\item how much of the past should be used to predict the next observation (length of weights) and
\item the values of the weights.
\end{enumerate}

Three common models of time series data are \textit{autoregressive} (AR) models, the \textit{integrated} (I) models, and the \textit{moving average} (MA) models.
These three classes depend linearly on previous data points and are combined in the autoregressive integrated moving average (ARIMA) model.
A model of this form is referred to as an $ARIMA(p,d,q)$ model where $p,d,q \in \mathbb{N^*}$.
The order of the autoregressive part is $p$, the order of the integrated part is $d$, and the order of the moving average part is $q$.
Given a time series of data $X_t$ (where $t$ is integer valued and the $X_t$ are real numbers) then an $ARIMA(p,d,q)$ model is given by
\begin{equation}
\left(1 - \sum_{i=1}^p \phi_i L^i\right) (1-L)^d X_t =
\left(1 + \sum_{i=1}^q \theta_i L^i\right) \varepsilon_t\,
\end{equation}
where $L$ is the lag operator,
$\phi$ are the parameters of the autoregressive part of the model,
$\theta$ are the parameters of the moving average part,
$d \in \mathbb{N^*}$ (if instead we have $d=0$ then this model is equivalent to an ARMA model),
and the $\epsilon_t$ are error terms.
The error terms $\epsilon_t$ are generally assumed to be independent and identically distributed variables sampled from a normal distribution with zero mean:
$\epsilon_t \sim N(0,\sigma^2)$ where $\sigma^2$ is the variance.
ARIMA models are commonly used for predicting and analyzing simpler time series.
They have been used on the stock market, but are generally viewed only as an indicator, not a predictive tool, due to the complexity of the market and because of their need for accurate knowledge about the time series itself.
It is for similar reasons that most traditional statistical methods fail to be of any real use in this task.

For example
\begin{equation}
y(t)= \frac{y(t-3)}{3} + \frac{y(t-2)}{3} + \frac{y(t-1)}{3}
\end{equation}
is a potential ARIMA model; another potential ARIMA model is
\begin{equation}
y(t)= \frac{y(t-3)}{6} + \frac{4 y(t-2)}{6} + \frac{y(t-1)}{6}.
\end{equation}
The correct ARIMA model requires identification of the right number of lags and the coefficients that should be used.  ARIMA model identification uses autoregressions to identify the underling model.  Care must be taken to robustly identify and estimate parameters as outliers (pulses, level shifts, local time trends) can wreak havoc.

\subsection{Artificial Neural Networks}
An artificial neural network is a graph of connected processing elements called neurons which can exhibit complex global behavior as determined by the connections between the neurons and their parameters.
This technique was originally inspired by the examination of the central nervous systems of living creatures, most notably that of humans, the most significant information processing system found in nature.
While a neural network is not adaptive itself, most practical examples use algorithms designed to alter the weights of the connections in the network to produce a desired signal flow.
These networks are also similar to their biological counterparts in that their functions are performed collectively in parallel by the entire network, with no clear delineation of sub-tasks to which various units are assigned.
Modern artificial neural networks often abandon much of this for a more practical approach based on statistics and signal processing \cite{HK:2000:ANN}.
There have been many attempts to predict financial time series with artificial neural networks
\cite{WuLu:1993:CSC93, KR:1994:NNAPL},
and there have even been somewhat successful results using genetic algorithms to evolve the weights for neural networks \cite{SC:2002:NNGA, KCM:2005:GECCO2005}.
However, there is one main drawback that comes with the use of artificial neural networks.
There is no easy way to translate the neural network that has been produced into an understandable set of rules describing its innate knowledge: the information is effectively trapped in the weights on the neurons.
Extracting useful rules from ANN's is a challenging field unto itself \cite{AndrewsGeva:2000}.

\subsection{Non-LCS Evolutionary Approaches}
There have been attempts at using evolutionary approaches other than LCS's to predict and analyze markets and other time series, ranging from the simplistic to the very complex.
In \cite{FGLG:2006:GECCO}, traditional genetic algorithms were used to optimize the exact numbers to be used in traditional technical analysis.
In \cite{Belford:2006:GECCO}, traditional genetic algorithms were again used, but this time in optimizing the rule sets for candlestick-style analysis; this outperformed a random trader.
In \cite{Kaboudan:2000:CompEcon}, a simplified variant on the concept of genetic programming, coded in C++, was used to develop trading rules for six stocks, and they managed to return better results than both the market and a naive trader.
However, the innate challenges of the real market have lead many researchers to resort to simulated markets, whose simplicity can make fundamental discoveries about economic theory sometimes less challenging to achieve; a small survey of these sorts of markets can be found in \cite{WHD:2002:AIReview}.

\subsection{LCS-Based Approaches}
There have been a few attempts at using LCS's to analyze and predict financial markets.
We will highlight a few derived from XCS here, since the system presented here is also derived from XCS.

\subsubsection{XCS}
A predictive system lacking a memory component is almost completely useless in attempting to model a highly interdependent nonlinear multivariate time series such as the stock market with any hope of utility;
none the less, it has been attempted.
One of the more notable attempts at this is described in \cite{SchulenburgRoss:1996:LNAI}, in which an XCS was used to predict the correct trading action for a stock on consecutive trading days.
Later work by Schulenburg and Ross in \cite{SchulenburgRoss:2002:LETS} does show some promise:
they utilize the opinions of a large host of simulated traders in order to make a decision.
This would yield in the general vicinity of 9\%p.a. returns: not spectacular or applicable to real-world trading, but respectable.

\subsubsection{XCSF}
In \cite{Wilson:2001:GECCO} Wilson outlined an extension to XCS for the approximation of functions, called XCSF, which attempts to learn a function of the form $y=f(x)$,
where $y \in \mathbb{R}$, $|x|=n$, and $x_i \in \mathbb{Z} \forall x_i \in x$.
A classifier consists of $n$ interval predicates of the form $int_i = (l_i, u_i)$
and matches an input $x$ if and only if $l_i \le x_i \le u_i \forall i \in \mathbb{N}$. 
Classical two-point crossover is employed, but where crossover may occur in-between the alleles or at the ends of the prediction, although the action is not involved in the crossover process.
A covering classifier is generated for a situation $x$ by forming the $l_i$ through subtracting from $x_i$ some random integer from $[0,r_0]$, and forming $u_i$ by adding some other random integer from $[0,r_0]$ to $x_i$, both limited to a maximum range of possible input, where $r_0$ is a parameter.
A rule $r^1$ can subsume a rule $r^2$ if and only if $l^1_i \le l^2_i \land u^2_i \le u^1_i \forall i$.  
While this could possibly be used to predict some very simplistic time series data, function approximation often does not perform very well in real-world problems, as is well-known in reinforcement learning literature \cite{BoyanMoore:2004,PerkinsPrecup:2002},
and this drawback of XCSF (and similar approaches) is explicitly acknowledged in \cite{LLWG:2005:GECCO}.
This would be most definitely true of a system as complex as the stock market, which cannot be easily and usefully mapped to any polynomial.
